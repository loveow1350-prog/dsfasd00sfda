"""
Evaluation Report Generator
Generates evaluation reports in various formats
"""
import json
from pathlib import Path
from typing import Optional

from src.models import EvaluationReport
from src.utils import setup_logger

logger = setup_logger(__name__)


class ReportGenerator:
    """Generate evaluation reports"""

    def save_all_formats(
        self,
        report: EvaluationReport,
        output_dir: Optional[Path],
        pdf_name: Optional[str] = None
    ):
        """
        Save report in all formats

        Args:
            report: EvaluationReport to save
            output_dir: Output directory
            pdf_name: Original PDF name (optional)
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # Generate base filename
        if pdf_name:
            base_name = f"{pdf_name}_{report.document_id}"
        else:
            base_name = report.document_id

        # 1. Save JSON
        json_path = output_path / f"{base_name}_evaluation_report.json"
        self._save_json(report, json_path)
        logger.info(f"Saved JSON report: {json_path}")

        # 2. Save Markdown checklist
        md_path = output_path / f"{base_name}_evaluation_checklist.md"
        self._save_checklist_markdown(report, md_path)
        logger.info(f"Saved Markdown checklist: {md_path}")

        # 3. Save detailed feedback
        feedback_path = output_path / f"{base_name}_evaluation_feedback.md"
        self._save_detailed_feedback(report, feedback_path)
        logger.info(f"Saved detailed feedback: {feedback_path}")

    def _save_json(self, report: EvaluationReport, path: Path):
        """Save report as JSON"""
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(report.model_dump(), f, ensure_ascii=False, indent=2)

    def _save_checklist_markdown(self, report: EvaluationReport, path: Path):
        """Save checklist as Markdown"""
        md = self.generate_checklist_markdown(report)
        with open(path, 'w', encoding='utf-8') as f:
            f.write(md)

    def _save_detailed_feedback(self, report: EvaluationReport, path: Path):
        """Save detailed feedback as Markdown"""
        md = self.generate_detailed_feedback(report)
        with open(path, 'w', encoding='utf-8') as f:
            f.write(md)

    def generate_checklist_markdown(self, report: EvaluationReport) -> str:
        """
        Generate checklist markdown

        Returns:
            Markdown string with checklist format
        """
        md = f"""# í‰ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸ (Evaluation Checklist)

**ë¬¸ì„œ ID**: `{report.document_id}`  
**í‰ê°€ ì¼ì‹œ**: {report.timestamp}  
**ì´ì **: **{report.total_score:.1f}** / 100  
**ì „ì²´ í†µê³¼ìœ¨**: {report.overall_pass_rate * 100:.1f}%

---

"""

        for category in report.categories:
            # Category header
            md += f"\n## {category.category}"

            if category.weight > 0:
                md += f" (ê°€ì¤‘ì¹˜: {category.weight * 100:.0f}%, ì ìˆ˜: {category.score:.1f}ì )"

            md += f"\n\n**í†µê³¼**: {category.pass_count}/{category.total_count} " \
                  f"({category.pass_rate * 100:.1f}%)\n\n"

            # Checklist items
            for item in category.checklist_items:
                status = "âœ…" if item.result else "âŒ"
                confidence_emoji = "ðŸŸ¢" if item.confidence >= 0.7 else "ðŸŸ¡" if item.confidence >= 0.4 else "ðŸ”´"

                md += f"{status} **{item.subcategory}**: {item.question}\n"
                md += f"   - **íŒë‹¨** ({confidence_emoji} {item.confidence:.2f}): {item.reasoning}\n"

                if item.evidence:
                    md += f"   - **ê·¼ê±°**:\n"
                    for evidence in item.evidence[:3]:  # Limit to 3
                        truncated = evidence[:150] + "..." if len(evidence) > 150 else evidence
                        md += f"     - {truncated}\n"

                if item.search_results:
                    md += f"   - **ê²€ìƒ‰ ê²°ê³¼** ({len(item.search_results)}ê°œ):\n"
                    for result in item.search_results[:2]:
                        title = result.get('title', 'No title')[:80]
                        url = result.get('url', '')
                        md += f"     - [{title}]({url})\n"

                md += "\n"

        # Summary
        md += "\n---\n\n## ðŸ“Š ìš”ì•½\n\n"
        md += f"- **í‰ê°€ í•­ëª© ìˆ˜**: {report.summary['total_items']}ê°œ\n"
        md += f"- **í†µê³¼ í•­ëª© ìˆ˜**: {report.summary['total_passed']}ê°œ\n"
        md += f"- **ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜**:\n"

        for cat_name, cat_score in report.summary['category_scores'].items():
            md += f"  - {cat_name}: {cat_score:.1f}ì \n"

        return md

    def generate_detailed_feedback(self, report: EvaluationReport) -> str:
        """
        Generate detailed feedback report

        Returns:
            Markdown string with detailed feedback
        """
        md = f"""# ìƒì„¸ í‰ê°€ í”¼ë“œë°± (Detailed Evaluation Feedback)

**ë¬¸ì„œ ID**: `{report.document_id}`  
**í‰ê°€ ì¼ì‹œ**: {report.timestamp}  
**ì´ì **: **{report.total_score:.1f}** / 100

---

## ðŸŽ¯ ì¢…í•© í‰ê°€

ì´ {report.summary['total_items']}ê°œ í•­ëª© ì¤‘ {report.summary['total_passed']}ê°œë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤ \
({report.overall_pass_rate * 100:.1f}%).

"""

        # Strengths
        md += "\n### âœ… ê°•ì \n\n"
        passed_items = []
        for category in report.categories:
            for item in category.checklist_items:
                if item.result:
                    passed_items.append((category.category, item))

        if passed_items:
            for cat_name, item in passed_items[:5]:  # Top 5
                md += f"- **[{cat_name}] {item.subcategory}**: {item.reasoning}\n"
        else:
            md += "- í†µê³¼í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.\n"

        # Weaknesses
        md += "\n### âŒ ê°œì„  í•„ìš” ì‚¬í•­\n\n"
        if report.recommendations:
            for rec in report.recommendations[:10]:  # Top 10
                md += f"- {rec}\n"
        else:
            md += "- ëª¨ë“  í•­ëª©ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤!\n"

        # Category breakdown
        md += "\n---\n\n## ðŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„\n\n"

        for category in report.categories:
            md += f"\n### {category.category}\n\n"

            if category.weight > 0:
                md += f"**ê°€ì¤‘ì¹˜**: {category.weight * 100:.0f}%  \n"
                md += f"**íšë“ ì ìˆ˜**: {category.score:.1f} / {category.weight * 100:.1f}  \n"

            md += f"**í†µê³¼ìœ¨**: {category.pass_rate * 100:.1f}% ({category.pass_count}/{category.total_count})\n\n"

            # Failed items first
            failed = [item for item in category.checklist_items if not item.result]
            if failed:
                md += "**âŒ ë¯¸í†µê³¼ í•­ëª©**:\n\n"
                for item in failed:
                    md += f"- **{item.subcategory}**: {item.question}\n"
                    md += f"  - {item.reasoning}\n"
                    if item.evidence:
                        md += f"  - ê·¼ê±°: {item.evidence[0][:100]}...\n"
                    md += "\n"

            # Passed items
            passed = [item for item in category.checklist_items if item.result]
            if passed:
                md += "**âœ… í†µê³¼ í•­ëª©**:\n\n"
                for item in passed:
                    md += f"- **{item.subcategory}**: {item.reasoning}\n"

        # Recommendations
        md += "\n---\n\n## ðŸ’¡ ê°œì„  ê¶Œìž¥ì‚¬í•­\n\n"

        if report.recommendations:
            priority_recs = self._prioritize_recommendations(report)

            md += "### ìš°ì„ ìˆœìœ„ ë†’ìŒ\n\n"
            for rec in priority_recs['high']:
                md += f"1. {rec}\n"

            if priority_recs['medium']:
                md += "\n### ìš°ì„ ìˆœìœ„ ì¤‘ê°„\n\n"
                for rec in priority_recs['medium']:
                    md += f"- {rec}\n"
        else:
            md += "ëª¨ë“  í‰ê°€ í•­ëª©ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤! ðŸŽ‰\n"

        return md

    def _prioritize_recommendations(self, report: EvaluationReport) -> dict:
        """Prioritize recommendations by category weight"""
        high = []
        medium = []

        for rec in report.recommendations:
            # Check if from high-weight category
            if any(cat in rec for cat in ['ì£¼ì œ ì„ ì •', 'ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„', 'ë¬¸ì œ í•´ê²°ë ¥']):
                high.append(rec)
            else:
                medium.append(rec)

        return {'high': high[:5], 'medium': medium[:5]}
