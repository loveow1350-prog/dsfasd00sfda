# Evaluation ì „ìš© ëª¨ë¸ ì„¤ì • ì™„ë£Œ

## âœ… êµ¬í˜„ ì™„ë£Œ

### ì¶”ê°€ëœ ê¸°ëŠ¥

**LLMRouterì— evaluation ì „ìš© ëª¨ë¸ ì§€ì›**
- í‰ê°€ ì‘ì—…ì— ë³„ë„ì˜ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥
- í° ëª¨ë¸/ì‘ì€ ëª¨ë¸ê³¼ ë…ë¦½ì ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥

---

## ğŸ”§ ë³€ê²½ ì‚¬í•­

### 1. config.yamlì— eval_model ì„¤ì • ì¶”ê°€

```yaml
llm:
  # Small model (Ollama)
  small_model_provider: "ollama"
  small_model: "gemma3:4b"
  
  # Big model (Google Gemini)
  big_model_provider: "google"
  big_model: "gemma-3-27b-it"
  
  # Evaluation model (NEW!)
  eval_model_provider: "google"
  eval_model: "gemini-2.5-flash"
```

**íŠ¹ì§•**:
- `eval_model`ì€ big_modelê³¼ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥
- providerë„ ë…ë¦½ì ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥
- ë¯¸ì„¤ì • ì‹œ big_model ì‚¬ìš© (fallback)

---

### 2. LLMRouterì— eval_llm ì¶”ê°€

```python
class LLMRouter:
    def __init__(self, config):
        self.small_llm = LLMClient(...)  # ì‘ì€ ëª¨ë¸
        self.big_llm = LLMClient(...)    # í° ëª¨ë¸
        self.eval_llm = LLMClient(...)   # í‰ê°€ ëª¨ë¸ (NEW!)
```

**ë¡œì§**:
- eval_modelì´ big_modelê³¼ ë‹¤ë¥´ë©´ â†’ ë³„ë„ LLMClient ìƒì„±
- eval_model == big_modelì´ë©´ â†’ big_llm ì¬ì‚¬ìš©
- dual_model ë¹„í™œì„±í™” ì‹œ â†’ ë‹¨ì¼ ëª¨ë¸ ì‚¬ìš©

---

### 3. chat_eval() ë©”ì„œë“œ ì¶”ê°€

```python
def chat_eval(
    self,
    messages: List[Dict[str, str]],
    response_format: str = "text"
) -> str:
    """Evaluation ì „ìš© chat (í•­ìƒ eval_llm ì‚¬ìš©)"""
    logger.debug("Using eval model for evaluation task")
    return self.eval_llm.generate(user_msg, system_msg)
```

**íŠ¹ì§•**:
- `complexity` íŒŒë¼ë¯¸í„° ì—†ìŒ (í•­ìƒ eval_llm ì‚¬ìš©)
- JSON ëª¨ë“œ ìë™ ì²˜ë¦¬
- í‰ê°€ ì „ìš© ë¡œê¹…

---

### 4. ëª¨ë“  Evaluatorì—ì„œ chat_eval ì‚¬ìš©

**ìˆ˜ì •ëœ íŒŒì¼**:
- âœ… `algorithm_evaluator.py`
- âœ… `topic_evaluator.py`
- âœ… `data_evaluator.py`
- âœ… `problem_solving_evaluator.py`
- âœ… `plan_evaluator.py`

**Before**:
```python
response = self.llm.chat(
    messages=[...],
    response_format="json",
    complexity="high"  # big_llm ì‚¬ìš©
)
```

**After**:
```python
response = self.llm.chat_eval(
    messages=[...],
    response_format="json"  # eval_llm ì‚¬ìš©
)
```

---

## ğŸ“Š ëª¨ë¸ ì‚¬ìš© ë¶„ë°°

### í˜„ì¬ ì„¤ì • ì˜ˆì‹œ

```yaml
# config/config.yaml
small_model: "gemma3:4b"           # Ollama (ë¡œì»¬)
big_model: "gemma-3-27b-it"        # Gemini 27B
eval_model: "gemini-2.5-flash"     # Gemini 2.5 Flash
```

### ì‘ì—…ë³„ ëª¨ë¸ ì‚¬ìš©

| ì‘ì—… ìœ í˜• | ëª¨ë¸ | ì‚¬ìš©ì²˜ |
|---------|------|--------|
| **í‚¤ì›Œë“œ ì¶”ì¶œ** | gemma3:4b | search_client, problem_analyzer |
| **ê°„ë‹¨í•œ ë¶„ë¥˜** | gemma3:4b | structure_parser |
| **ë¬¸ì„œ êµ¬ì¡° íŒŒì‹±** | gemma-3-27b-it | structure_parser |
| **ë¬¸ì œ ë¶„ì„** | gemma-3-27b-it | problem_analyzer |
| **VLM ì‘ì—…** | gemma-3-27b-it | pdf_extractor |
| **í”„ë¡œì íŠ¸ í‰ê°€** â­ | gemini-2.5-flash | evaluators (NEW!) |

---

## ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: í‰ê°€ë§Œ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©

```yaml
big_model: "gemma-3-27b-it"      # ë©”ì¸ ì‘ì—…: ê°•ë ¥í•˜ì§€ë§Œ ëŠë¦¼
eval_model: "gemini-2.5-flash"   # í‰ê°€: ë¹ ë¥´ê³  íš¨ìœ¨ì 
```

**ì¥ì **:
- ë©”ì¸ íŒŒì´í”„ë¼ì¸: ì •í™•ë„ ìš°ì„ 
- í‰ê°€: ì†ë„ ìš°ì„ 

### ì‹œë‚˜ë¦¬ì˜¤ 2: ëª¨ë‘ ë™ì¼ ëª¨ë¸

```yaml
big_model: "gemini-2.5-flash"
eval_model: "gemini-2.5-flash"   # ë˜ëŠ” ë¯¸ì„¤ì •
```

**ì¥ì **:
- ì¼ê´€ëœ í’ˆì§ˆ
- ì„¤ì • ë‹¨ìˆœ

### ì‹œë‚˜ë¦¬ì˜¤ 3: í‰ê°€ì— ë” ê°•ë ¥í•œ ëª¨ë¸

```yaml
big_model: "gemini-2.5-flash"
eval_model: "gemini-1.5-pro"     # í‰ê°€ì— ë” ê°•ë ¥í•œ ëª¨ë¸
```

**ì¥ì **:
- í‰ê°€ ì •í™•ë„ ê·¹ëŒ€í™”

---

## ğŸš€ ì‹¤í–‰ ë¡œê·¸

í‰ê°€ ì‹¤í–‰ ì‹œ ë‹¤ìŒ ë¡œê·¸ê°€ í‘œì‹œë©ë‹ˆë‹¤:

```
INFO - Initializing dual-model strategy
INFO - Small model: ollama/gemma3:4b
INFO - Big model: google/gemma-3-27b-it
INFO - Eval model: google/gemini-2.5-flash  â† NEW!

DEBUG - Using eval model for evaluation task  â† í‰ê°€ ì‹œ
```

---

## ğŸ¯ í•µì‹¬ ì´ì 

### 1. ìœ ì—°ì„±
- í‰ê°€ì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ ê°€ëŠ¥
- providerë„ ë…ë¦½ì ìœ¼ë¡œ ì„¤ì •

### 2. ë¹„ìš© ìµœì í™”
- ë©”ì¸: ê°•ë ¥í•œ ëª¨ë¸ (ì •í™•ë„)
- í‰ê°€: ë¹ ë¥¸ ëª¨ë¸ (íš¨ìœ¨ì„±)

### 3. ì„±ëŠ¥ ìµœì í™”
- í‰ê°€ëŠ” ë¹ ë¥¸ ëª¨ë¸ë¡œ ì‹ ì† ì²˜ë¦¬
- ë©”ì¸ ì‘ì—…ì€ ëŠë ¤ë„ ì •í™•í•˜ê²Œ

### 4. í•˜ìœ„ í˜¸í™˜ì„±
- eval_model ë¯¸ì„¤ì • ì‹œ big_model ì‚¬ìš©
- ê¸°ì¡´ ì„¤ì • ê·¸ëŒ€ë¡œ ì‘ë™

---

## ğŸ“ ì„¤ì • ì˜ˆì‹œ

### ì˜ˆì‹œ 1: Gemini Flash (í‰ê°€ ì „ìš©)

```yaml
eval_model_provider: "google"
eval_model: "gemini-2.5-flash"
```

### ì˜ˆì‹œ 2: GPT-3.5 (ë¹„ìš© ì ˆê°)

```yaml
big_model_provider: "google"
big_model: "gemini-1.5-pro"

eval_model_provider: "openai"
eval_model: "gpt-3.5-turbo"
```

### ì˜ˆì‹œ 3: Ollama (ì™„ì „ ë¬´ë£Œ)

```yaml
eval_model_provider: "ollama"
eval_model: "gemma2:9b"
eval_model_base_url: "http://localhost:11434"
```

---

## âœ¨ ìš”ì•½

**3ê°œì˜ ë…ë¦½ì ì¸ ëª¨ë¸ ì„¤ì •**:
1. `small_model` - ê°„ë‹¨í•œ ì‘ì—… (í‚¤ì›Œë“œ, ë¶„ë¥˜)
2. `big_model` - ë³µì¡í•œ ì‘ì—… (íŒŒì‹±, ë¶„ì„, VLM)
3. `eval_model` - í‰ê°€ ì „ìš© (NEW!) â­

**ëª¨ë“  evaluatorê°€ `chat_eval()` ì‚¬ìš©**:
- âœ… ì¼ê´€ëœ í‰ê°€ ëª¨ë¸ ì‚¬ìš©
- âœ… ë…ë¦½ì ì¸ ì„¤ì • ê°€ëŠ¥
- âœ… í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€

---

**êµ¬í˜„ ì™„ë£Œ ì¼ì‹œ**: 2026-02-01 15:15  
**ì˜í–¥ ë²”ìœ„**: evaluation ì‹œìŠ¤í…œ ì „ì²´  
**í•˜ìœ„ í˜¸í™˜ì„±**: ìœ ì§€ (eval_model ë¯¸ì„¤ì • ì‹œ big_model ì‚¬ìš©)
